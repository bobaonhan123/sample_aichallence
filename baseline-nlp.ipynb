{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: C√†i ƒë·∫∑t th∆∞ vi·ªán (ch·∫°y cell n√†y tr∆∞·ªõc ti√™n)\n",
        "!pip install -q transformers datasets torch sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Import v√† test c∆° b·∫£n\n",
        "from transformers import pipeline\n",
        "print(\"Th∆∞ vi·ªán ƒë√£ s·∫µn s√†ng!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Sentiment Analysis (Ph√¢n lo·∫°i c·∫£m x√∫c) - Baseline si√™u nhanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "texts = [\n",
        "    \"S·∫£n ph·∫©m n√†y tuy·ªát v·ªùi qu√°!\",\n",
        "    \"T·ªá h·∫°i, ƒë·ª´ng mua\",\n",
        "    \"B√¨nh th∆∞·ªùng th√¥i, kh√¥ng ƒë·∫∑c bi·ªát\"\n",
        "]\n",
        "\n",
        "results = classifier(texts)\n",
        "for text, res in zip(texts, results):\n",
        "    print(f\"C√¢u: {text}\")\n",
        "    print(f\"K·∫øt qu·∫£: {res['label']} (ƒë·ªô tin c·∫≠y: {res['score']:.4f})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Named Entity Recognition (NER) - Nh·∫≠n di·ªán th·ª±c th·ªÉ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
        "\n",
        "text = \"Elon Musk sinh s·ªëng ·ªü Texas. Apple ƒëang mua m·ªôt startup ·ªü Anh v·ªõi gi√° 1 t·ª∑ ƒë√¥.\"\n",
        "\n",
        "results = ner(text)\n",
        "for entity in results:\n",
        "    print(f\"Th·ª±c th·ªÉ: {entity['word']} - Lo·∫°i: {entity['entity_group']} - ƒê·ªô tin c·∫≠y: {entity['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Summarization (T√≥m t·∫Øt vƒÉn b·∫£n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "article = \"\"\"\n",
        "X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) l√† m·ªôt lƒ©nh v·ª±c c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o t·∫≠p trung v√†o s·ª± t∆∞∆°ng t√°c gi·ªØa m√°y t√≠nh v√† con ng∆∞·ªùi b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n. \n",
        "Trong nh·ªØng nƒÉm g·∫ßn ƒë√¢y, c√°c m√¥ h√¨nh transformer nh∆∞ BERT v√† GPT ƒë√£ c√°ch m·∫°ng h√≥a lƒ©nh v·ª±c n√†y. \n",
        "C√°c hackathon th∆∞·ªùng y√™u c·∫ßu x√¢y d·ª±ng prototype nhanh b·∫±ng c√°c m√¥ h√¨nh pre-trained t·ª´ Hugging Face.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarizer(article, max_length=60, min_length=20, do_sample=False)\n",
        "print(\"T√≥m t·∫Øt:\", summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Question Answering (Tr·∫£ l·ªùi c√¢u h·ªèi theo ng·ªØ c·∫£nh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "context = \"Hugging Face l√† c√¥ng ty ·ªü New York chuy√™n t·∫°o c√¥ng c·ª• NLP m√£ ngu·ªìn m·ªü. C√¥ng ty ƒë∆∞·ª£c th√†nh l·∫≠p nƒÉm 2016.\"\n",
        "question = \"Hugging Face ƒë∆∞·ª£c th√†nh l·∫≠p nƒÉm n√†o?\"\n",
        "\n",
        "answer = qa(question=question, context=context)\n",
        "print(f\"C√¢u h·ªèi: {question}\")\n",
        "print(f\"Tr·∫£ l·ªùi: {answer['answer']} (ƒë·ªô tin c·∫≠y: {answer['score']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Text Generation (Sinh vƒÉn b·∫£n) - D√πng model nh·ªè ƒë·ªÉ nhanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "prompt = \"Trong m·ªôt hackathon NLP, c√°ch th·∫Øng nhanh nh·∫•t l√†\"\n",
        "\n",
        "output = generator(prompt, max_length=60, num_return_sequences=1, temperature=0.7)\n",
        "print(\"K·∫øt qu·∫£ sinh vƒÉn b·∫£n:\")\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Demo nhanh m·ªôt app ƒë∆°n gi·∫£n b·∫±ng Gradio (n·∫øu mu·ªën c√≥ giao di·ªán ƒë·∫πp ƒë·ªÉ tr√¨nh b√†y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "    result = sentiment_pipe(text)[0]\n",
        "    label = \"T√≠ch c·ª±c üòä\" if result['label'] == 'POSITIVE' else \"Ti√™u c·ª±c üò°\" if result['label'] == 'NEGATIVE' else \"Trung l·∫≠p üòê\"\n",
        "    return label, round(result['score'], 4)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=sentiment_analysis,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Nh·∫≠p c√¢u c·∫ßn ph√¢n t√≠ch c·∫£m x√∫c...\"),\n",
        "    outputs=[gr.Textbox(label=\"C·∫£m x√∫c\"), gr.Textbox(label=\"ƒê·ªô tin c·∫≠y\")],\n",
        "    title=\"Demo Sentiment Analysis cho Hackathon\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)  # S·∫Ω t·∫°o link c√¥ng khai ƒë·ªÉ b·∫°n demo cho ban gi√°m kh·∫£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ch√∫c b·∫°n ch·∫°y m∆∞·ª£t v√† th·∫Øng l·ªõn ·ªü hackathon nh√©! üî•\n",
        "\n",
        "N·∫øu task c·ªßa b·∫°n li√™n quan ƒë·∫øn ti·∫øng Vi·ªát nhi·ªÅu, m√¨nh c√≥ th·ªÉ g·ª£i √Ω th√™m model ti·∫øng Vi·ªát (Phobert, VinAI) n·ªØa nha!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
